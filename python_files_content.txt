路径：read_py_files.py

内容：
import os

def read_py_files():
    # 获取当前目录
    current_dir = os.getcwd()
    
    # 用于存储所有内容的列表
    all_content = []
    
    # 遍历所有目录和文件
    for root, dirs, files in os.walk(current_dir):
        for file in files:
            if file.endswith('.py'):
                # 获取完整的文件路径
                file_path = os.path.join(root, file)
                # 获取相对路径
                rel_path = os.path.relpath(file_path, current_dir)
                
                try:
                    # 读取文件内容
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # 添加路径和内容到列表
                    all_content.append(f"路径：{rel_path}\n")
                    all_content.append(f"内容：\n{content}\n")
                    all_content.append("-" * 80 + "\n")  # 分隔线
                except Exception as e:
                    print(f"读取文件 {rel_path} 时出错：{str(e)}")
    
    # 将内容写入txt文件
    with open('python_files_content.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(all_content))

if __name__ == "__main__":
    read_py_files()
    print("文件内容已保存到 python_files_content.txt") 

--------------------------------------------------------------------------------

路径：sjpl_scraper/sjpl_scraper/__init__.py

内容：


--------------------------------------------------------------------------------

路径：sjpl_scraper/sjpl_scraper/middlewares.py

内容：


--------------------------------------------------------------------------------

路径：sjpl_scraper/sjpl_scraper/settings.py

内容：
BOT_NAME = "sjpl_scraper"

SPIDER_MODULES = ["sjpl_scraper.spiders"]
NEWSPIDER_MODULE = "sjpl_scraper.spiders"


--------------------------------------------------------------------------------

路径：sjpl_scraper/sjpl_scraper/items.py

内容：
import scrapy

class SjplEventItem(scrapy.Item):
    event_id = scrapy.Field()
    title = scrapy.Field()
    start_time = scrapy.Field()
    end_time = scrapy.Field()
    location = scrapy.Field()
    location_details = scrapy.Field()
    audiences = scrapy.Field()
    event_types = scrapy.Field()
    languages = scrapy.Field()
    description = scrapy.Field()
    link = scrapy.Field()
    is_cancelled = scrapy.Field()

--------------------------------------------------------------------------------

路径：sjpl_scraper/sjpl_scraper/pipelines.py

内容：


--------------------------------------------------------------------------------

路径：sjpl_scraper/sjpl_scraper/spiders/sjpl_events.py

内容：
import scrapy
import json
from bs4 import BeautifulSoup
from sjpl_scraper.items import SjplEventItem

class SjplEventsSpider(scrapy.Spider):
    name = "sjpl_events"
    allowed_domains = ["gateway.bibliocommons.com"]
    start_urls = ["https://gateway.bibliocommons.com/v2/libraries/sjpl/events/search?page=1&limit=20&locale=en-US"]

    custom_settings = {
        'FEEDS': {
            'sjpl_events.csv': {
                'format': 'csv',
                'encoding': 'utf-8-sig',
                'overwrite': True
            },
        },
    }

    HEADERS = {
        'accept': 'application/json',
        'accept-language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'origin': 'https://sjpl.bibliocommons.com',
        'referer': 'https://sjpl.bibliocommons.com/',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36',
    }

    COOKIES = {
        'EVENT': 'app01b',
        'NERF_SRV': 'nerf17',
        'SRV': 'app36',
        '_ga': 'GA1.1.184796718.1749706354',
        'branch': '{"ip":"128.199.11.195","sjpl":null}',
        '_ga_F5QPDQX1BM': 'GS2.1.s1750055591$o6$g1$t1750057934$j46$l0$h0',
        '_ga_G99DMMNG39': 'GS2.1.s1750055591$o5$g1$t1750057934$j46$l0$h0'
    }

    def start_requests(self):
        yield scrapy.Request(self.start_urls[0], headers=self.HEADERS, cookies=self.COOKIES, callback=self.parse, meta={'page': 1})

    def parse(self, response):
        data = json.loads(response.text)
        entities = data.get('entities', {})
        locations = {k: v['name'] for k, v in entities.get('locations', {}).items()}
        audiences = {k: v['name'] for k, v in entities.get('eventAudiences', {}).items()}
        event_types = {k: v['name'] for k, v in entities.get('eventTypes', {}).items()}
        languages = {k: v['name'] for k, v in entities.get('eventLanguages', {}).items()}

        for event_id in data.get('events', {}).get('results', []):
            event_data = entities.get('events', {}).get(event_id)
            if not event_data:
                continue
            definition = event_data.get('definition', {})
            description_html = definition.get('description', '')
            soup = BeautifulSoup(description_html, 'html.parser')
            cleaned_description = soup.get_text(separator='\n', strip=True)

            location_id = definition.get('branchLocationId')
            if event_data.get('isVirtual'):
                location_name = "Online / Virtual"
            elif location_id:
                location_name = locations.get(location_id, f"Unknown ID: {location_id}")
            else:
                location_name = "N/A"

            item = SjplEventItem()
            item['event_id'] = event_id
            item['title'] = definition.get('title', 'N/A')
            item['start_time'] = definition.get('start', 'N/A')
            item['end_time'] = definition.get('end', 'N/A')
            item['location'] = location_name
            item['location_details'] = definition.get('locationDetails', '')
            item['audiences'] = ', '.join([audiences.get(aid, 'N/A') for aid in definition.get('audienceIds', [])])
            item['event_types'] = ', '.join([event_types.get(tid, 'N/A') for tid in definition.get('typeIds', [])])
            item['languages'] = ', '.join([languages.get(lid, 'N/A') for lid in definition.get('languageIds', [])])
            item['description'] = cleaned_description
            item['link'] = f"https://sjpl.bibliocommons.com/events/{event_id}"
            item['is_cancelled'] = definition.get('isCancelled', False)
            yield item

        pagination = data.get('pagination', {})
        current_page = pagination.get('page')
        total_pages = pagination.get('pages')
        if current_page and total_pages and current_page < total_pages:
            next_page = current_page + 1
            next_url = f"https://gateway.bibliocommons.com/v2/libraries/sjpl/events/search?page={next_page}&limit=20&locale=en-US"
            yield scrapy.Request(next_url, headers=self.HEADERS, cookies=self.COOKIES, callback=self.parse, meta={'page': next_page})

--------------------------------------------------------------------------------
